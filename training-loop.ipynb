{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attnetion block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x) # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attnetion block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x) # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3) # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import time\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortended context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, tokenizer, text_to_token_ids, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "        \n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1) # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
    "        \n",
    "        # Otherwise, same as before: get the idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True) # (batch_size, 1)\n",
    "        \n",
    "        if idx_next == eos_id: # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "        \n",
    "        # if idx_next == text_to_token_ids(\".\", tokenizer):\n",
    "        if idx_next == \"tensor([[13]])\":\n",
    "            # idx_next = idx_next + text_to_token_ids(\"Meow.\", tokenizer)\n",
    "            print(\"\\nperiod\\n\")\n",
    "        \n",
    "        # if idx_next == text_to_token_ids(\"?\", tokenizer):\n",
    "        if idx_next == \"tensor([[30]])\":\n",
    "            # idx_next = idx_next + text_to_token_ids(\"Meow.\", tokenizer)\n",
    "            print(\"\\nperiod\\n\")\n",
    "        \n",
    "        # if idx_next == text_to_token_ids(\"!\", tokenizer):\n",
    "        if idx_next == \"tensor([[0]])\":\n",
    "            # idx_next = idx_next + text_to_token_ids(\"Meow.\", tokenizer)\n",
    "            print(\"\\nperiod\\n\")\n",
    "        \n",
    "        # print(idx_next)\n",
    "        # print(\"----\")\n",
    "        # print(idx_next + text_to_token_ids(\"Meow.\", tokenizer))\n",
    "        # test = idx_next + text_to_token_ids(\"Meow.\", tokenizer)\n",
    "        # print(\"------\")\n",
    "        # print(token_ids_to_text(idx_next, tokenizer))\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (batch_size, num_tokens+1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_to_token_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtext_to_token_ids\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_to_token_ids' is not defined"
     ]
    }
   ],
   "source": [
    "print(text_to_token_ids(\"!\", tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, device,\n",
    "                n_epochs, eval_freq, eval_iter, start_context, tokenizer,\n",
    "                warmup_steps, initial_lr=3e-05, min_lr=1e-6):\n",
    "    \n",
    "    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Retrieve the maximum learning rate from the optimizer\n",
    "    peak_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    # Calculate the total number of iterations in the training process\n",
    "    total_training_steps = len(train_loader) * n_epochs\n",
    "\n",
    "    # Calculate the learning rate increment during the warmup phase\n",
    "    lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            # Adjust the learning rate based on the current phase (warmup or cosine annealing)\n",
    "            if global_step < warmup_steps:\n",
    "                # Linear warmup\n",
    "                lr = initial_lr + global_step * lr_increment\n",
    "            else:\n",
    "                # Cosine annealing after warmup\n",
    "                progress = ((global_step - warmup_steps) /\n",
    "                            (total_training_steps - warmup_steps))\n",
    "                lr = min_lr + (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "            \n",
    "            # Apply the calculated learning rate to the optimizer\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "            track_lrs.append(lr) # Store the current learning rate\n",
    "\n",
    "            # Calculate and backpropagate the loss\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "\n",
    "            # Apply gradient clipping after the warmup phase to avoid exploding gradients\n",
    "            if global_step > warmup_steps:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "\n",
    "            # Periodically evaluate the model on the training and validation sets\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader,\n",
    "                    device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                # Print the current losses\n",
    "                print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        # Generate and print a sample from the model to monitor progress\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen, track_lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") # A - Initalize the tokenizer\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # B - Create dataset\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last, # C - drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
    "        num_workers=0 # D - The number of CPU processes to use for preprocessing\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt) # A\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride): # B\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i +max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5308,  322,   13]])\n",
      "tensor([[13]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(text_to_token_ids(\"Meow.\", tokenizer))\n",
    "print(text_to_token_ids(\".\", tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \")) # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # apply softmax to get the probabilities\n",
    "        probas = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True) # (batch, 1)\n",
    "\n",
    "        # if idx_next == text_to_token_ids(\".\", tokenizer):\n",
    "        #     idx_next = idx_next + text_to_token_ids(\"Meow.\", tokenizer)\n",
    "        \n",
    "        # if idx_next == text_to_token_ids(\"?\", tokenizer):\n",
    "        #     idx_next = idx_next + text_to_token_ids(\"Meow.\", tokenizer)\n",
    "        \n",
    "        # if idx_next == text_to_token_ids(\"!\", tokenizer):\n",
    "        #     idx_next = idx_next + text_to_token_ids(\"Meow.\", tokenizer)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (batch , n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "missingLinks = []\n",
    "for PG in range(1, 74439):\n",
    "\n",
    "    if PG < 10001:\n",
    "        if PG not in (10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 30, 31, 32, 33, 35, 36, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 54, 55, 57, 58, 59, 60, 61, 62, 64, 68, 70,71,72,73, \n",
    "74, \n",
    "76, \n",
    "77, \n",
    "78, \n",
    "81, \n",
    "82, \n",
    "83, \n",
    "84, \n",
    "85, \n",
    "86, \n",
    "90, \n",
    "91, \n",
    "92, \n",
    "93, \n",
    "94, \n",
    "95, \n",
    "96, \n",
    "97, \n",
    "98, \n",
    "99, \n",
    "114, \n",
    "129, \n",
    "182, \n",
    "183, \n",
    "184, \n",
    "185, \n",
    "186, \n",
    "187, \n",
    "188, \n",
    "189, \n",
    "190, \n",
    "191, \n",
    "192, \n",
    "193, \n",
    "194, \n",
    "195, \n",
    "196, \n",
    "197, \n",
    "198, \n",
    "199, \n",
    "239, \n",
    "256, \n",
    "462, \n",
    "628, \n",
    "629, \n",
    "630, \n",
    "631, \n",
    "632, \n",
    "635, \n",
    "664, \n",
    "745, \n",
    "758, \n",
    "900, \n",
    "928, \n",
    "1070, \n",
    "1071, \n",
    "1072, \n",
    "1073, \n",
    "1255, \n",
    "1316, \n",
    "1344, \n",
    "1460, \n",
    "1647, \n",
    "1648, \n",
    "1691, \n",
    "1723, \n",
    "1766, \n",
    "1767, \n",
    "1789, \n",
    "1856, \n",
    "1914, \n",
    "1939, \n",
    "1964, \n",
    "1984, \n",
    "2001, \n",
    "2091, \n",
    "2200, \n",
    "2201, \n",
    "2202, \n",
    "2203, \n",
    "2204, \n",
    "2205, \n",
    "2206, \n",
    "2207, \n",
    "2208, \n",
    "2209, \n",
    "2210, \n",
    "2211, \n",
    "2212, \n",
    "2213, \n",
    "2214, \n",
    "2215, \n",
    "2216, \n",
    "2217, \n",
    "2218, \n",
    "2219, \n",
    "2220, \n",
    "2221, \n",
    "2222, \n",
    "2223, \n",
    "2224, \n",
    "2367, \n",
    "2623, \n",
    "2624, \n",
    "2625, \n",
    "2626, \n",
    "2738, \n",
    "2770, \n",
    "2774, \n",
    "2877, \n",
    "2879, \n",
    "2994, \n",
    "3002, \n",
    "3018, \n",
    "3184, \n",
    "3185, \n",
    "3201, \n",
    "3445, \n",
    "3446, \n",
    "3448, \n",
    "3449, \n",
    "3450, \n",
    "3501, \n",
    "3502, \n",
    "3503, \n",
    "3504, \n",
    "3505, \n",
    "3506, \n",
    "3507, \n",
    "3508, \n",
    "3509, \n",
    "3510, \n",
    "3511, \n",
    "3513, \n",
    "3514, \n",
    "3515, \n",
    "3516, \n",
    "3517, \n",
    "3518, \n",
    "3519, \n",
    "3520, \n",
    "3521, \n",
    "3522, \n",
    "3523, \n",
    "3524, \n",
    "3651, \n",
    "3803, \n",
    "3926, \n",
    "4002, \n",
    "4107, \n",
    "4274, \n",
    "4405, \n",
    "4699, \n",
    "4749, \n",
    "4750, \n",
    "4751, \n",
    "4935, \n",
    "4936, \n",
    "4949, \n",
    "4950, \n",
    "4951, \n",
    "5001, \n",
    "5124, \n",
    "5188, \n",
    "5189, \n",
    "5190, \n",
    "5212, \n",
    "5213, \n",
    "5214, \n",
    "5215, \n",
    "5216, \n",
    "5373, \n",
    "5613, \n",
    "5627, \n",
    "5634, \n",
    "5635, \n",
    "5714, \n",
    "5740, \n",
    "5884, \n",
    "5885, \n",
    "5886, \n",
    "6084, \n",
    "6191, \n",
    "6302, \n",
    "6532, \n",
    "6533, \n",
    "6534, \n",
    "6535, \n",
    "6536, \n",
    "6537, \n",
    "6538, \n",
    "6539, \n",
    "6540, \n",
    "6541, \n",
    "6542, \n",
    "6543, \n",
    "6544, \n",
    "6546, \n",
    "6547, \n",
    "6548, \n",
    "6550, \n",
    "6551, \n",
    "6552, \n",
    "6553, \n",
    "6554, \n",
    "6555, \n",
    "6556, \n",
    "6557, \n",
    "6620, \n",
    "6871, \n",
    "6951, \n",
    "7092, \n",
    "7093, \n",
    "7094, \n",
    "7507, \n",
    "7536, \n",
    "7684, \n",
    "7825, \n",
    "7869, \n",
    "7872, \n",
    "7873, \n",
    "7874, \n",
    "8204, \n",
    "8205, \n",
    "8227, \n",
    "8608, \n",
    "8609, \n",
    "8610, \n",
    "8611, \n",
    "8612, \n",
    "8613, \n",
    "8614, \n",
    "8615, \n",
    "8616, \n",
    "8617, \n",
    "8618, \n",
    "8619, \n",
    "8620, \n",
    "8621, \n",
    "8622, \n",
    "8623, \n",
    "8624, \n",
    "8625, \n",
    "8626, \n",
    "8627, \n",
    "8628, \n",
    "8629, \n",
    "8630, \n",
    "8631, \n",
    "8632, \n",
    "8633, \n",
    "8634, \n",
    "8635, \n",
    "8636, \n",
    "8637, \n",
    "8746, \n",
    "8748, \n",
    "8749, \n",
    "8750, \n",
    "8751, \n",
    "8752, \n",
    "8753, \n",
    "8754, \n",
    "8755, \n",
    "8756, \n",
    "8757, \n",
    "8758, \n",
    "8759, \n",
    "8760, \n",
    "8761, \n",
    "8762, \n",
    "8763, \n",
    "8764, \n",
    "8765, \n",
    "8766, \n",
    "8767, \n",
    "8768, \n",
    "8769, \n",
    "8806, \n",
    "8807, \n",
    "8808, \n",
    "8809, \n",
    "8810, \n",
    "8811, \n",
    "8812, \n",
    "8816, \n",
    "8817, \n",
    "8818, \n",
    "8958, \n",
    "8959, \n",
    "8960, \n",
    "8962, \n",
    "8963, \n",
    "8965, \n",
    "8966, \n",
    "8967, \n",
    "8968, \n",
    "8969, \n",
    "8970, \n",
    "8971, \n",
    "8972, \n",
    "8973, \n",
    "8974, \n",
    "8975, \n",
    "8976, \n",
    "8977, \n",
    "8978, \n",
    "8979, \n",
    "8980, \n",
    "8981, \n",
    "8982, \n",
    "8983, \n",
    "8984, \n",
    "8985, \n",
    "8986, \n",
    "8987, \n",
    "8988, \n",
    "8989, \n",
    "8990, \n",
    "9001, \n",
    "9002, \n",
    "9003, \n",
    "9004, \n",
    "9005, \n",
    "9006, \n",
    "9007, \n",
    "9008, \n",
    "9009, \n",
    "9010, \n",
    "9011, \n",
    "9012, \n",
    "9013, \n",
    "9014, \n",
    "9015, \n",
    "9016, \n",
    "9017, \n",
    "9018, \n",
    "9019, \n",
    "9020, \n",
    "9021, \n",
    "9022, \n",
    "9023, \n",
    "9024, \n",
    "9025, \n",
    "9026, \n",
    "9027, \n",
    "9028, \n",
    "9029, \n",
    "9030, \n",
    "9031, \n",
    "9032, \n",
    "9033, \n",
    "9034, \n",
    "9035, \n",
    "9036, \n",
    "9037, \n",
    "9038, \n",
    "9039, \n",
    "9040, \n",
    "9041, \n",
    "9042, \n",
    "9056, \n",
    "9113, \n",
    "9114, \n",
    "9115, \n",
    "9116, \n",
    "9117, \n",
    "9118, \n",
    "9119, \n",
    "9120, \n",
    "9121, \n",
    "9122, \n",
    "9123, \n",
    "9124, \n",
    "9125, \n",
    "9126, \n",
    "9127, \n",
    "9128, \n",
    "9129, \n",
    "9130, \n",
    "9131, \n",
    "9132, \n",
    "9133, \n",
    "9134, \n",
    "9135, \n",
    "9136, \n",
    "9137, \n",
    "9138, \n",
    "9139, \n",
    "9140, \n",
    "9141, \n",
    "9142, \n",
    "9143, \n",
    "9144, \n",
    "9145, \n",
    "9146, \n",
    "9147, \n",
    "9255, \n",
    "9268, \n",
    "9269, \n",
    "9270, \n",
    "9271, \n",
    "9272, \n",
    "9273, \n",
    "9274, \n",
    "9275, \n",
    "9276, \n",
    "9277, \n",
    "9278, \n",
    "9279, \n",
    "9280, \n",
    "9281, \n",
    "9282, \n",
    "9283, \n",
    "9284, \n",
    "9285, \n",
    "9286, \n",
    "9287, \n",
    "9288, \n",
    "9289, \n",
    "9290, \n",
    "9291, \n",
    "9292, \n",
    "9293, \n",
    "9336, \n",
    "9337, \n",
    "9338, \n",
    "9339, \n",
    "9340, \n",
    "9341, \n",
    "9342, \n",
    "9343, \n",
    "9344, \n",
    "9345, \n",
    "9346, \n",
    "9347, \n",
    "9348, \n",
    "9349, \n",
    "9350, \n",
    "9351, \n",
    "9352, \n",
    "9353, \n",
    "9354, \n",
    "9355, \n",
    "9356, \n",
    "9357, \n",
    "9358, \n",
    "9359, \n",
    "9360, \n",
    "9361, \n",
    "9392, \n",
    "9416, \n",
    "9417, \n",
    "9418, \n",
    "9419, \n",
    "9420, \n",
    "9421, \n",
    "9422, \n",
    "9423, \n",
    "9424, \n",
    "9425, \n",
    "9426, \n",
    "9427, \n",
    "9428, \n",
    "9429, \n",
    "9430, \n",
    "9431, \n",
    "9432, \n",
    "9433, \n",
    "9434, \n",
    "9435, \n",
    "9436, \n",
    "9437, \n",
    "9438, \n",
    "9451, \n",
    "9452, \n",
    "9510, \n",
    "9511, \n",
    "9512, \n",
    "9513, \n",
    "9514, \n",
    "9515, \n",
    "9516, \n",
    "9517, \n",
    "9518, \n",
    "9519, \n",
    "9520, \n",
    "9521, \n",
    "9522, \n",
    "9523, \n",
    "9524, \n",
    "9525, \n",
    "9526, 9527, 9528, 9529, 9530, 9531, 9532, 9533, 9534, 9535, 9536, 9537, 9538, 9539, 9540, 9541,9551,9552, 9553, \n",
    "9554, 9555, \n",
    "9556, \n",
    "9557, \n",
    "9558, \n",
    "9671, \n",
    "9672, \n",
    "9673, 9674, 9675, 9676, 9677, 9678, 9679, 9680, \n",
    "9681, \n",
    "9682, 9683, 9684, 9685, 9686, 9687, 9688, 9689, 9690, 9691, 9692, 9693, 9694, 9695, 9696, 9697, 9698, 9699, 9702, 9703, 9704, 9705, 9706, 9707, 9708, 9709, 9710, 9711, 9712, 9713, 9714, 9715, 9716, 9717, 9718, 9719, 9720, 9721, 9722, 9723, 9724, 9725, 9726, 9727, 9728, 9729, \n",
    "9730, 9731, 9732, 9733, 9734, 9735, 9736, 9737, 9738, 9739, 9740, 9741, 9742, 9743, 9744, 9830, 9930, 9933, 9934, 9942,):\n",
    "\n",
    "            # print(\"--------------------\")\n",
    "            # print(f\"PG {PG}\")\n",
    "            import os\n",
    "            import urllib.request\n",
    "\n",
    "            file_path = f\"text/PG{PG}_text.txt\"\n",
    "            url = \"https://huggingface.co/datasets/KittyCat00/CatGPTDataset/raw/main/PG\" + str(PG) + \"_text.txt\"\n",
    "\n",
    "            # # # if not os.path.exists(file_path):\n",
    "            # # #     with urllib.request.urlopen(url) as response:\n",
    "            # # #         text_data = response.read().decode('utf-8')\n",
    "            # # #     with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            # # #         file.write(text_data)\n",
    "            # # # else:\n",
    "            # # #     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            # # #         text_data = file.read()\n",
    "            # with urllib.request.urlopen(url) as response:\n",
    "            #     text_data = response.read().decode('utf-8')\n",
    "            # with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            #     file.write(text_data)\n",
    "\n",
    "            missingLinks = []\n",
    "\n",
    "\n",
    "            # import module\n",
    "            from urllib.request import urlopen\n",
    "            from urllib.error import *\n",
    " \n",
    "            # try block to read URL\n",
    "            try:\n",
    "                html = urlopen(url)\n",
    "     \n",
    "            # except block to catch\n",
    "            # exception\n",
    "            # and identify error\n",
    "            except HTTPError as e:\n",
    "                # print(\"HTTP error\", e)\n",
    "                print(str(PG) + \", \")\n",
    "                missingLinks.append(PG)\n",
    "                \n",
    "            except URLError as e:\n",
    "                # print(\"Opps ! Page not found!\", e)\n",
    "                print(str(PG) + \", \")\n",
    "                missingLinks.append(PG)\n",
    "                \n",
    "            \n",
    "           \n",
    "print(missingLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "PG 25\n",
      "Output text:\n",
      " Every effort moves you, IU, UN.935%, US$1,\n",
      "\n",
      "\n",
      "Output text:\n",
      " Every effort moves you, IU, UN.935%, US$1,\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[324], line 59\u001b[0m\n\u001b[1;32m     49\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     52\u001b[0m     },\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_and_optimizer.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     57\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_and_optimizer.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 59\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGPT_CONFIG_124M\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     62\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[303], line 10\u001b[0m, in \u001b[0;36mGPTModel.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m\"\u001b[39m], cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_emb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrf_blocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;241m*\u001b[39m[\u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm \u001b[38;5;241m=\u001b[39m LayerNorm(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\n\u001b[1;32m     15\u001b[0m     cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m], cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n",
      "Cell \u001b[0;32mIn[304], line 5\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43md_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memb_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43md_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memb_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_heads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqkv_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff \u001b[38;5;241m=\u001b[39m FeedForward(cfg)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1 \u001b[38;5;241m=\u001b[39m LayerNorm(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[305], line 14\u001b[0m, in \u001b[0;36mMultiHeadAttention.__init__\u001b[0;34m(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_query \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_in, d_out, bias\u001b[38;5;241m=\u001b[39mqkv_bias)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_key \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_in, d_out, bias\u001b[38;5;241m=\u001b[39mqkv_bias)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_value \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqkv_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_out, d_out) \u001b[38;5;66;03m# Linear layer to combine head outputs\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(dropout)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:104\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:110\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv/lib/python3.12/site-packages/torch/nn/init.py:460\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    458\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for PG in range(1, 74439):\n",
    "\n",
    "    if PG < 10001 and PG > 24 :\n",
    "        if PG not in (10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 30, 31, 32, 33, 35, 36, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 54, 55, 57, 58, 59, 60, 61, 62, 64, 68):\n",
    "\n",
    "            print(\"--------------------\")\n",
    "            print(f\"PG {PG}\")\n",
    "\n",
    "            model.to(\"cpu\")\n",
    "            model.eval()\n",
    "\n",
    "            tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "            torch.manual_seed(123)\n",
    "\n",
    "            token_ids = generate(\n",
    "                model=model,\n",
    "                idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "                max_new_tokens=15,\n",
    "                context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "                top_k=25,\n",
    "                temperature=1.4,\n",
    "                text_to_token_ids=text_to_token_ids,\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "\n",
    "            print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "\n",
    "            torch.manual_seed(123)\n",
    "\n",
    "            token_ids = generate(\n",
    "                model=model,\n",
    "                idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "                max_new_tokens=15,\n",
    "                context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "                top_k=25,\n",
    "                temperature=1.4,\n",
    "                text_to_token_ids=text_to_token_ids,\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "\n",
    "            print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "                \"model_and_optimizer.pth\"\n",
    "            )\n",
    "\n",
    "\n",
    "            checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "            model = GPTModel(GPT_CONFIG_124M)\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            model.train()\n",
    "\n",
    "\n",
    "            from importlib.metadata import version\n",
    "\n",
    "\n",
    "            print(\"torch version:\", version(\"torch\"))\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device(\"cuda\")\n",
    "            elif torch.backends.mps.is_available():\n",
    "                device = torch.device(\"mps\")\n",
    "            else:\n",
    "                device = torch.device(\"cpu\")\n",
    "\n",
    "            print(f\"Using {device} device.\")\n",
    "\n",
    "            torch.manual_seed(123)\n",
    "            model = GPTModel(GPT_CONFIG_124M)\n",
    "            model.eval()\n",
    "\n",
    "\n",
    "            import os\n",
    "            import urllib.request\n",
    "\n",
    "            file_path = f\"text/PG{PG}_text.txt\"\n",
    "            url = \"https://huggingface.co/datasets/KittyCat00/CatGPTDataset/raw/main/PG\" + str(PG) + \"_text.txt\"\n",
    "\n",
    "            # if not os.path.exists(file_path):\n",
    "            #     with urllib.request.urlopen(url) as response:\n",
    "            #         text_data = response.read().decode('utf-8')\n",
    "            #     with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            #         file.write(text_data)\n",
    "            # else:\n",
    "            #     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            #         text_data = file.read()\n",
    "            with urllib.request.urlopen(url) as response:\n",
    "                text_data = response.read().decode('utf-8')\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(text_data)\n",
    "\n",
    "\n",
    "            # Train/validation ratio\n",
    "            train_ratio = 0.90\n",
    "            split_idx = int(train_ratio * len(text_data))\n",
    "\n",
    "\n",
    "            torch.manual_seed(123)\n",
    "\n",
    "            train_loader = create_dataloader_v1(\n",
    "                text_data[:split_idx],\n",
    "                batch_size=2,\n",
    "                max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "                stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "                drop_last=True,\n",
    "                shuffle=True,\n",
    "                num_workers=0\n",
    "            )\n",
    "\n",
    "            val_loader = create_dataloader_v1(\n",
    "                text_data[split_idx:],\n",
    "                batch_size=2,\n",
    "                max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "                stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "                drop_last=False,\n",
    "                shuffle=False,\n",
    "                num_workers=0\n",
    "            )\n",
    "\n",
    "\n",
    "            n_epochs = 15\n",
    "            initial_lr = 0.0001\n",
    "            peak_lr = 0.01\n",
    "\n",
    "\n",
    "            total_steps = len(train_loader) * n_epochs\n",
    "            warmup_steps = int(0.2 * total_steps) # 20% warmup\n",
    "            print(warmup_steps)\n",
    "\n",
    "\n",
    "            import tiktoken\n",
    "            import time\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            torch.manual_seed(123)\n",
    "            model = GPTModel(GPT_CONFIG_124M)\n",
    "            model.to(device)\n",
    "\n",
    "            peak_lr = 5e-4\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\n",
    "            tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "            n_epochs = 15\n",
    "            train_losses, val_losses, tokens_seen, lrs = train_model(\n",
    "                model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,\n",
    "                eval_freq=5, eval_iter=1, start_context=\"Every effort moves you\",\n",
    "                tokenizer=tokenizer, warmup_steps=warmup_steps,\n",
    "                initial_lr=1e-5, min_lr=1e-5\n",
    "            )\n",
    "\n",
    "            end_time = time.time()\n",
    "            execution_time_minutes = (end_time - start_time) / 60\n",
    "            print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you), minimized Windsorarian Engelicone dose proven signalingfly Finn screenshots bald EXT charm\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    text_to_token_ids=text_to_token_ids,\n",
    "    top_k=25,\n",
    "    tokenizer=tokenizer,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"torch version:\", version(\"torch\"))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/AlanikREDAWN/CatGPT/main/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    text_data[:split_idx],\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    text_data[split_idx:],\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "initial_lr = 0.0001\n",
    "peak_lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_loader) * n_epochs\n",
    "warmup_steps = int(0.2 * total_steps) # 20% warmup\n",
    "print(warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "peak_lr = 5e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "n_epochs = 15\n",
    "train_losses, val_losses, tokens_seen, lrs = train_model(\n",
    "    model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,\n",
    "    eval_freq=5, eval_iter=1, start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer, warmup_steps=warmup_steps,\n",
    "    initial_lr=1e-5, min_lr=1e-5\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
